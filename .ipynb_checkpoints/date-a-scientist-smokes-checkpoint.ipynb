{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b29b3f60-d752-423d-8909-68f28302db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f06347f9-d0a5-4d59-988e-02505c0ae221",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[356], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiles.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1456\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\multiarray.py:1131\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \n\u001b[0;32m   1172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c906fa-0366-4239-9075-97deb5d1d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f9685-2e63-4f49-ba12-2c8423c0cb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d9c5-d0ce-4f2d-9da8-f08fbd48ceab",
   "metadata": {},
   "source": [
    "Input variables: age, body_type, diet, drinks, drugs, education, ethnicity, height, job, offspring, orientation, pets, religion, sex, sign, smokes, \n",
    "\n",
    "I am not incuding the 'essay' columns, because these are very hard to quantify. I could use text classification to categorize them and quentify them, but that will be for another projects\n",
    "I am not incuding the 'speaks' column, because there are too many values with too little frequency, which could lead to overfitting.\n",
    "\n",
    "\n",
    "Target values: Status misschien wel het meest interessante, aangezien dit het doel is van de app. Ongeveer 4,2% van vrouwen is seeing someone en 3,4% van de mannen. \n",
    "Zou het nog kunnen uitsplitsen per man/vrouwe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed0ad8-3b15-45be-a99a-d2a830d1efb2",
   "metadata": {},
   "source": [
    "Lets keep it simple and only use 'job', 'age' and 'sex' to predict 'income'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622542a5-a203-4f53-9bfd-8144fdd85d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57623749-3759-4b10-b667-1dcf003c0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram\n",
    "data['age'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ff1a6-830f-498b-a7d7-66172e7dab99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram\n",
    "data['income'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "\n",
    "plt.title('Income Distribution')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ee500-6629-431f-9eea-05c35f6b9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.income.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36349c48-9167-42d5-8d0d-6d44e44efd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_above_0 = data[data['income'] > 0 ]\n",
    "len(income_above_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0dc52-9455-46df-b793-9b45ad365511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram\n",
    "income_above_0['income'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "\n",
    "plt.title('Income Distribution')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec0f1d-2115-4dea-a018-6c4c89b31769",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_above_0['income'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7ffb6-4204-43d4-8c77-16296aca5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_above_0_below_150k = income_above_0[income_above_0['income'] < 150000]\n",
    "# Plot histogram\n",
    "income_above_0_below_150k['income'].plot(kind='hist', bins=30, edgecolor='black')\n",
    "\n",
    "plt.title('Income Distribution')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6bc10-4817-49e9-9f44-c87d253b9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_above_0_below_150k['income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc7b3b-ae39-4371-b641-3aa4f4331100",
   "metadata": {},
   "source": [
    "Lets keep it simple and only use 'job', 'age' and 'sex' to predict 'income'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d7109-2ab7-4231-bef6-9bf95f9fe4d9",
   "metadata": {},
   "source": [
    "## Predicting income by using nn regression and linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e8ddf-d419-4bde-a82f-512dee435fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_select = income_above_0_below_150k[['job', 'age', 'sex', 'income', 'smokes', 'offspring', 'body_type', 'diet','drugs', 'drinks','education']]\n",
    "\n",
    "data_select = pd.get_dummies(\n",
    "    data_select,\n",
    "    columns = ['job', 'sex', 'smokes', 'offspring', 'body_type', 'diet', 'drugs','drinks', 'education'], # I could manually code the values for drink and drug frequency\n",
    "    dtype = int\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713a450-ec0f-4350-a0ce-d617f1776301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for all columns\n",
    "corr_matrix = data_select.corr()\n",
    "\n",
    "# Extract correlations of all features with the target column\n",
    "corr_with_target = corr_matrix['income'].drop('income')  # drop self-correlation\n",
    "\n",
    "# Sort correlations by absolute value descending\n",
    "corr_with_target_sorted = corr_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "print(corr_with_target_sorted.head(20))\n",
    "print(corr_with_target_sorted.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9903e-8687-495e-8dc0-d4f4014dc454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_col_list = [col for col in data_select.columns if col != 'income']\n",
    "y_col_list = ['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164db8a-d335-4283-9426-5425378eb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = data_select[X_col_list]\n",
    "y_scaled = scaler.fit_transform(data_select[y_col_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbc6ef-5763-4bc6-aa3d-a47cef6cb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tensors\n",
    "import torch\n",
    "X_tensor = torch.tensor(X.values, dtype = torch.float)\n",
    "y_tensor = torch.tensor(y_scaled, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec9670-dc78-4b34-ae33-6920235d4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tensor_train, X_tensor_test, y_tensor_train, y_tensor_test = train_test_split(X_tensor, y_tensor, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d2095-9cd1-475c-97c0-b7a106b75ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980e826-f447-4db7-813f-088fe3a4128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a7e9a-f021-4b59-b136-720438053fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80867a-0080-46c3-a1f1-4ae2534c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08394de-4441-4465-9ac2-dd1b5677cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "nn_regmodel = nn.Sequential(\n",
    "    nn.Linear(114,114),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(114,48),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(48,24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927b245-eba9-48c7-8712-d6dcc1c34731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "import torch.optim as optim\n",
    "\n",
    "loss = MSELoss()\n",
    "optimizer = optim.Adam(nn_regmodel.parameters(), lr= 0.08)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = nn_regmodel(X_tensor_train)\n",
    "    MSE = loss(predictions, y_tensor_train)\n",
    "    \n",
    "\n",
    "    MSE.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "     \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], MSELoss: {MSE.item():.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e72618-fd5f-4754-a174-3a8ba1eb7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_regmodel(X_tensor_train)\n",
    "loss = MSELoss()\n",
    "mse = loss(predictions, y_tensor_train)\n",
    "rmse = torch.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0241e-255d-4c3f-88d4-342797450874",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg_model = nn.Sequential(\n",
    "    nn.Linear(114, 114),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(114,48),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(48,24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24,1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.L1Loss()  # MAE = Mean Absolute Error\n",
    "optimizer = optim.Adam(nn_reg_model.parameters(), lr= 0.01)\n",
    "\n",
    "num_epochs = 100 #DECREASED \n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = nn_reg_model(X_tensor_train)\n",
    "    MAE = loss_fn(predictions, y_tensor_train)\n",
    "    \n",
    "\n",
    "    MAE.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "     \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], MAELoss: {MAE.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04d821-cb0b-4190-922b-82ff70227eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(nn_reg_model, 'nn_regression_y_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd53b48-42b1-4ba7-911d-16be23612f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Evaluating\n",
    "# nn_regression_y_scaled = torch.load('nn_regression_y_scaled', weights_only=False)\n",
    "# MAEloss = nn.L1Loss()\n",
    "# MSEloss = MSEloss()\n",
    "\n",
    "# #Calculating the Mean Average Error\n",
    "# nn_regression_y_scaled.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = nn_regression_y_scaled(X_tensor_train)\n",
    "#     train_MAE = MAEloss(predictions, y_tensor_train)\n",
    "\n",
    "# nn_regression_y_scaled.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = nn_regression_y_scaled(X_tensor_train)\n",
    "#     train_MSE = MSEloss(predictions, y_tensor_train)\n",
    "\n",
    "\n",
    "# ### What does this do really????\n",
    "# import math\n",
    "# train_MSE # = 896,490,000\n",
    "# train_RMSE = math.sqrt(test_MSE) #  29,941.460752608582\n",
    "\n",
    "# print(train_MSE)\n",
    "# print(train_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b58eb-bea3-4e3e-8ca7-fd6174662655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "nn_regression_y_scaled = torch.load('nn_regression_y_scaled', weights_only=False)\n",
    "MAEloss = nn.L1Loss()\n",
    "MSEloss = MSELoss()\n",
    "\n",
    "#Calculating the Mean Average Error\n",
    "nn_regression_y_scaled.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = nn_regression_y_scaled(X_tensor_test)\n",
    "    test_MAE = MAEloss(predictions, y_tensor_test)\n",
    "\n",
    "#Calculating the Mean Squared Error\n",
    "nn_regression_y_scaled.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = nn_regression_y_scaled(X_tensor_test)\n",
    "    test_MSE = MSEloss(predictions, y_tensor_test)\n",
    "\n",
    "#Taking the root for interpretability\n",
    "import math\n",
    "test_RMSE = math.sqrt(test_MSE) \n",
    "\n",
    "print(test_MAE)\n",
    "print(test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52666fa1-e89d-4f0f-b65a-6b00df3d9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Suppose y is a 1D NumPy array or Series\n",
    "y = data_select[y_col_list].values.reshape(-1, 1)  # ensure it's 2D\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)  # now scaler_y is fitted!\n",
    "\n",
    "# Now this will work:\n",
    "mae_original = 0.6947 * scaler_y.scale_[0]\n",
    "rmse_original = 0.9789 * scaler_y.scale_[0]\n",
    "\n",
    "print(f\"MAE (original scale): {mae_original:.2f}\")\n",
    "print(f\"RMSE (original scale): {rmse_original:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415cedc-7bd3-4001-b398-312134ed5533",
   "metadata": {},
   "source": [
    "A mean average loss of 20,168.40 with a data set with the minimum of 20.000 and a maximum of 100.000 isn't that good. I wonder how a simple multiple linear regression would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f6774-5430-41bc-b9bb-f5747b5e2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data_select[X_col_list]\n",
    "y = data_select[y_col_list]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "print(lm.score(X_train, y_train))\n",
    "print(lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0c4fe-480c-4caa-a81d-293ddde1ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = data_select[X_col_list]\n",
    "y = data_select[y_col_list]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n",
    "lm = LinearRegression()\n",
    "\n",
    "#Train model\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on train data\n",
    "predictions = lm.predict(X_train)\n",
    "\n",
    "#Calculate loss on train data\n",
    "loss_train = mean_absolute_error(y_train, predictions)\n",
    "\n",
    "#Make predictions on test data\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "#Calculate loss on test data\n",
    "loss_test = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "#print results\n",
    "print(loss_train)\n",
    "print(loss_test)\n",
    "print(lm.score(X_train, y_train))\n",
    "print(lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a05b5-856c-4305-be21-7656752c4207",
   "metadata": {},
   "source": [
    "# Predicting smokers non-smokers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9860aae-d863-4ffd-adf4-ab9547806986",
   "metadata": {},
   "source": [
    "I assign all the smoker values, from 'yes' (I smoke) to 'when drinking' to '1' and the non-smoker value 'no' to 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "2144b4d5-8bf4-4398-a257-b200ffa21bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_6160\\2877692.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_select['smokes'] = data_select['smokes'].replace({'yes': 1, 'trying to quit':1, 'sometimes': 1, 'when drinking': 1, 'no':0})\n",
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_6160\\2877692.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_select['smokes'] = data_select['smokes'].replace({'yes': 1, 'trying to quit':1, 'sometimes': 1, 'when drinking': 1, 'no':0})\n"
     ]
    }
   ],
   "source": [
    "data_select = income_above_0_below_150k[['age','smokes']]\n",
    "\n",
    "# data_select = pd.get_dummies(\n",
    "#     data_select,\n",
    "#     columns = ['job'], # I could manually code the values for drink and drug frequency\n",
    "#     dtype = int\n",
    "# )\n",
    "data_select['smokes'] = data_select['smokes'].replace({'yes': 1, 'trying to quit':1, 'sometimes': 1, 'when drinking': 1, 'no':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "760c3912-8593-4f87-9a5c-82fea80e69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_select['smokes'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "75db54c9-601f-480b-bd9a-105aaf837e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59917</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59927</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59930</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59934</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59943</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10155 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age\n",
       "1       35\n",
       "3       23\n",
       "11      28\n",
       "13      30\n",
       "14      29\n",
       "...    ...\n",
       "59917   26\n",
       "59927   26\n",
       "59930   41\n",
       "59934   43\n",
       "59943   42\n",
       "\n",
       "[10155 rows x 1 columns]"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_col_list = [col for col in data_select.columns if col != 'smokes']\n",
    "y_col_list = ['smokes']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nn_bin_X = data_select[X_col_list]\n",
    "nn_bin_y= data_select[y_col_list]\n",
    "\n",
    "nn_bin_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "f9ecb678-ff07-4db5-8106-eddbc87f59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(nn_bin_X.values, dtype = torch.float)\n",
    "y_tensor = torch.tensor(nn_bin_y.values, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "e2c67a85-b134-4f40-ae6e-bdd8adcbc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "nn_bin_X_train, nn_bin_X_test, nn_bin_y_train, nn_bin_X_test = train_test_split(X_tensor, y_tensor, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "7be6bca6-e22e-4fe3-9af5-36a3dcc93f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: nan\n",
      "Epoch [200/1000], Loss: nan\n",
      "Epoch [300/1000], Loss: nan\n",
      "Epoch [400/1000], Loss: nan\n",
      "Epoch [500/1000], Loss: nan\n",
      "Epoch [600/1000], Loss: nan\n",
      "Epoch [700/1000], Loss: nan\n",
      "Epoch [800/1000], Loss: nan\n",
      "Epoch [900/1000], Loss: nan\n",
      "Epoch [1000/1000], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "nn_bin_classifier = nn.Sequential(\n",
    "    nn.Linear(1,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(nn_bin_classifier.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = nn_bin_classifier(nn_bin_X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(predictions, nn_bin_y_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Only check prediction range at the first epoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "dbd1f46a-3e04-4387-9958-e54d3a2e261e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/300], Loss: nan\n",
      "Epoch [200/300], Loss: nan\n",
      "Epoch [300/300], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "mask = ~torch.isnan(nn_bin_y_train).squeeze()\n",
    "nn_bin_y_train = nn_bin_y_train[mask]\n",
    "nn_bin_X_train = nn_bin_X_train[mask]\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nn_bin_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: model outputs logits\n",
    "    logits = nn_bin_classifier(nn_bin_X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(logits, nn_bin_y_train)\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "8aae70aa-488f-46a4-9e20-b3b57cb4d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10155.000000\n",
      "mean        32.260758\n",
      "std          9.777148\n",
      "min         18.000000\n",
      "25%         25.000000\n",
      "50%         30.000000\n",
      "75%         37.000000\n",
      "max         69.000000\n",
      "Name: age, dtype: float64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(nn_bin_X['age'].describe())\n",
    "print(nn_bin_X['age'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "207f8c8f-d603-463b-9eec-2234433ad0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in inputs: False\n",
      "Infs in inputs: False\n",
      "NaNs in targets: False\n",
      "Infs in targets: False\n"
     ]
    }
   ],
   "source": [
    "print(\"NaNs in inputs:\", torch.isnan(nn_bin_X_train).any().item())\n",
    "print(\"Infs in inputs:\", torch.isinf(nn_bin_X_train).any().item())\n",
    "print(\"NaNs in targets:\", torch.isnan(nn_bin_y_train).any().item())\n",
    "print(\"Infs in targets:\", torch.isinf(nn_bin_y_train).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "c0f448d3-2968-409f-ad39-f1cb067ab91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "mask = ~torch.isnan(nn_bin_y_train).squeeze()\n",
    "nn_bin_y_train = nn_bin_y_train[mask]\n",
    "nn_bin_X_train = nn_bin_X_train[mask]\n",
    "print(nn_bin_y_train[torch.isnan(nn_bin_y_train)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "daeb9919-58ce-4e87-a310-9ebc529cd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8124, 1])\n",
      "torch.Size([7782, 1])\n",
      "tensor(nan, grad_fn=<MinBackward1>) tensor(nan, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(nn_bin_y_train.shape)\n",
    "print(predictions.min(), predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "dbcd45c1-2658-45fd-a2f8-096190aa10e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Before training:\", nn_bin_classifier(nn_bin_X_train[:5]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
